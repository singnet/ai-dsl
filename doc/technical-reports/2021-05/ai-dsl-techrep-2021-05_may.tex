%% Technical Report for the work on the AI-DSL over the period of
%% March to May 2021.

\documentclass[]{report}
\usepackage{url}
\usepackage{minted}
\usepackage[textsize=footnotesize]{todonotes}
\newcommand{\kabir}[2][]{\todo[color=yellow,author=kabir, #1]{#2}}
\newcommand{\nil}[2][]{\todo[color=purple,author=nil, #1]{#2}}
\usepackage{hyperref}
\usepackage{breakurl}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\footnotesize,breaklines=false,frame=single}
\usepackage{float}
\restylefloat{table}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{subcaption}

\begin{document}

\title{AI-DSL Technical Report (February to May 2021)}
\author{Nil Geisweiller, Kabir Veitas, Eman Shemsu Asfaw, Samuel Roberti}
\maketitle

\begin{abstract}
Based on~\cite{GoertzelGeisweillerBlog}.
\end{abstract}

\tableofcontents

\chapter{Nil's work}

Work done:
\begin{enumerate}
\item Implement \texttt{RealizedFunction} as described
  in~\cite{GoertzelGeisweillerBlog}.
\item Implement a network of trivially simple AI services implemented
  in Idris2, and use Idris compiler to type check if they can properly
  connect to each other.
\item Implement a Registry prototype, as a proof-of-concept for
  querying AI services based on their dependently typed
  specifications.
\end{enumerate}

\section{Realized Function}
\label{realized_function}

\subsection{Description}

The \texttt{RealizedFunction} data structure, as introduced
in~\cite{GoertzelGeisweillerBlog}, is a wrapper around a regular
function to integrate aspects of its specifications pertaining to its
execution on real physical substrates as opposed to just its
algorithmic properties.  For instance it contains descriptions of
costs (financial, computational, etc) and performances (quality, etc)
captured in the \texttt{RealizedAttributes} data structure, as
introduced in~\cite{GoertzelGeisweillerBlog} as well.

For that iteration we have implemented a simple version of
\texttt{RealizedFunction} and \texttt{RealizedAttributes} in
Idris2~\cite{Idris}.  The \texttt{RealizedAttributes} data structure
contains
\begin{itemize}
\item \texttt{Costs}: as a triple of three constants,
  \texttt{financial}, \texttt{temporal} and \texttt{computational},
\item \texttt{Quality}: as a single \texttt{quality} value.
\end{itemize}
as well as an example of compositional law,
\texttt{add\_costs\_min\_quality}, where costs are additive and
quality is infimum-itive.  Below is a small snippet of that code to
give an idea of how it looks like

\begin{minted}[mathescape]{idris}
record RealizedAttributes where
  constructor MkRealizedAttributes
  costs : Costs
  quality : Quality
\end{minted}

\begin{minted}[mathescape]{idris}
add_costs_min_quality : RealizedAttributes ->
                        RealizedAttributes ->
                        RealizedAttributes
add_costs_min_quality f_attrs g_attrs = fg_attrs where
  fg_attrs : RealizedAttributes
  fg_attrs = MkRealizedAttributes (add_costs f_attrs.costs g_attrs.costs)
                                  (min f_attrs.quality g_attrs.quality)
\end{minted}
The full implementation can be found in
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/realized-function/RealizedAttributes.idr}{\texttt{RealizedAttributes.idr}},
under the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/realized-function/}{\texttt{experimental/realized-function/}}
folder of the \href{https://github.com/singnet/ai-dsl/}{AI-DSL
  repository}~\cite{AIDSLRepo}.\\

Then we have implemented \texttt{RealizedFunction} that essentially
attaches a \texttt{RealizedAttributes} instance to a function.  In
addition we have implemented a composition (as in function
composition) operating on \texttt{RealizedFunction} instead of
regular function, making use of that compositional law above.
Likewise below is a snippet of that code

\begin{minted}[mathescape]{idris}
data RealizedFunction : (t : Type) -> (attrs : RealizedAttributes) -> Type where
  MkRealizedFunction : (f : t) -> (attrs : RealizedAttributes) ->
                       RealizedFunction t attrs
\end{minted}

\begin{minted}[mathescape]{idris}
compose : {a : Type} -> {b : Type} -> {c : Type} ->
          (RealizedFunction (b -> c) g_attrs) ->
          (RealizedFunction (a -> b) f_attrs) ->
          (RealizedFunction (a -> c) (add_costs_min_quality f_attrs g_attrs))
compose (MkRealizedFunction g g_attrs) (MkRealizedFunction f f_attrs) =
  MkRealizedFunction (g . f) (add_costs_min_quality f_attrs g_attrs)
\end{minted}
The full implementation can be found in
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/realized-function/RealizedFunction.idr}{\texttt{RealizedFunction.idr}}
under the same folder.

\subsection{Objectives and achievements}

The objectives of this work was to see if Idris2 was able to type
check that the realized attributes of composed realized functions
followed the defined compositional law.  We have found that Idris2 is
not only able to do that, but to our surprise much faster that Idris1
(instantaneous instead of seconds to minutes), by bypassing induction
on numbers and using efficient function-driven rewriting on the
realized attributes instead.  That experiment can be found in
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/realized-function/RealizedFunction-test.idr}{\texttt{RealizedFunction-test.idr}},
under the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/realized-function/}{\texttt{experimental/realized-function/}}
folder of the \href{https://github.com/singnet/ai-dsl/}{AI-DSL
  repository}~\cite{AIDSLRepo}.

\subsection{Future work}

Experimenting with constants as realized attributes was the first step
in our investigation.  The subsequent steps will be to replace
constants by functions, probability distributions and other
sophisticated ways to represent costs and quality.

\section{Network of Idris AI services}
\label{network_idris_ai_services}

\subsection{Description}

In this work we have implemented a small network of trivially simple
AI services, with the objective of testing if the Idris compiler could
be used to type check the validity of their connections. Three primary
services were implemented
\begin{enumerate}
\item \texttt{incrementer}: increment an integer by 1
\item \texttt{twicer}: multiply an integer by 2
\item \texttt{halfer}: divide an integer by 2
\end{enumerate}
as well as composite services based on these primary services, such as
\begin{itemize}
\item \texttt{incrementer . halfer . twicer}
\end{itemize}
with the objective of testing that such compositions were properly
typed.  The networking part was implemented based on the
SingularityNET example service~\cite{SNETExampleService} mentioned in
the SingularityNET tutorial~\cite{SNETTutorial}.  The specifics of
that implementation are of little importance for that report and thus
are largely ignored.  The point was to try to be as close as possible
to real networking conditions.  For the part that matters to us we may
mention that communications between AI services are handled by
gRPC~\cite{gRPC}, which has some level of type checking by insuring
that the data being exchanged fulfill some type structures (list of
integers, union type of string and bool, etc) specified in Protocol
Buffers~\cite{Protobuf}.  Thus one may see the usage of Idris in that
context as adding an enhanced refined verification layer on top of
gRPC making use of the expressive power of dependent types.

\subsection{Objectives and achievements}

As mentioned above the objectives of such an experiment was to see how
the Idris compiler can be used to type check combinations of AI
services.  It was initially envisioned to make use of dependent types
by specifying that the \texttt{twicer} service outputs an even
integer, as opposed to any integer, and that the \texttt{halfer}
service only accepts an even integer as well.  The idea was to
prohibit certain combinations such as
\begin{itemize}
\item \texttt{halfer . incrementer . twicer}
\end{itemize}
Since the output of \texttt{incrementer . twicer} is provably odd,
\texttt{halfer} should refuse it and such combination should be
rejected.  This objective was not reached in this experiment, but
similar objectives were reached other experiments, see Section
\ref{aidsl_registry}\nil{Add ref to Sam's work}.  The other objective
was to type check that the compositions have realized attributes
corresponding to the compositional law implemented in Section
\ref{realized_function}, which was fully achieved in this experiment.
For instance by changing either the types, costs or quality of the
following composition
\begin{minted}[mathescape]{idris}
-- Realized (twicer . incrementer).
rlz_compo1_attrs : RealizedAttributes
rlz_compo1_attrs = MkRealizedAttributes (MkCosts 300 30 3) 0.9
-- The following does not work because 301 /= 200+100
-- rlz_compo1_attrs = MkRealizedAttributes (MkCosts 301 30 3) 0.9
rlz_compo1 : RealizedFunction (Int -> Int) Compo1.rlz_compo1_attrs
rlz_compo1 = compose rlz_twicer rlz_incrementer
\end{minted}
defined in
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/simple-idris-services/service/Compo1.idr}{\texttt{experimental/simple-idris-services/service/Compo1.idr}},
the corresponding service would raise a type checking error at start
up.  More details on the experiment and how to run it can be found in
the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/simple-idris-services/README.md}{\texttt{README.md}}
under the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/simple-idris-services/}{\texttt{experimental/simple-idris-services/service/}}
folder of the \href{https://github.com/singnet/ai-dsl/}{AI-DSL
  repository}~\cite{AIDSLRepo}.

\subsection{Future work}

Such experiment was good to explore how Idris can be integrated to a
network of services.  What we need to do next is experiment with
actual AI algorithms, ideally making full use of dependent types in
their specifications.  Such endeavor was actually attempted over the
AI service collective described in Section
\ref{domain_model_considerations}, but it was eventually concluded to
be too ambitious for that iteration and was postponed for the next.

An intermediary step, before moving to actual AI algorithms, is to
explore multi-facet specifications and their interactions with other
AI services.  That is, a service A might output a data type satisfying
a combination of properties, such as, to tie that back to the trivial
incrementer, twicer and halfer combination, an integer that is both
even and within a certain interval.  Then two other services, B and C,
may have partial constraints instead.  For instance B may require an
even integer, while C may require that integer to be within a certain
interval.  In other words both can take as input the output of A, for
different reasons.  It's obviously not conceptual difficult to cast
the output of A to match the input types of B and C, however it is
still something we need to explore in practice with Idris.

Obviously we want to be able to reuse existing AI services and write
their enhanced specifications on top of them, as opposed to writing
both specification and code in Idris/AI-DSL.  To that end it was noted
that having a Protobuf to/from Idris/AI-DSL converter would be useful,
so that a developer can start from an existing AI service, specified
in Protobuf, and enriched it with dependent types in Idris/AI-DSL.
The other way around could be useful as well to enable a developer to
implement AI services entirely in Idris/AI-DSL and expose their
Protobuf specification to the network.  To that end having an
implementation of gRPC for Idris/AI-DSL could be handy as well.

\section{AI-DSL Registry}
\label{aidsl_registry}

\subsection{Description}

One important goal of the AI-DSL is to have a system that can perform
autonomous matching and composition of AI services, so that provided
the specification of an AI, it should suffice to find it, complete it
or even entirely build it from scratch.  We have implemented a
proof-of-concept \emph{registry} to start experimenting with such
functionalities.

So far we have two versions in the
\href{https://github.com/singnet/ai-dsl/}{AI-DSL repository}, one
without dependent types support, under
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/registry/}{\texttt{experimental/registry/}},
and a more recent one with dependent type support that can be found
under
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/registry-dtl/}{\texttt{experimental/registry-dtl/}}.
We will focus our attention on the latter which is far more
interesting.\\

The AI-DSL registry (reminiscent of the SingularityNET
registry~\cite{SNETRegistry}) is itself an AI service with the following functions
\begin{enumerate}
\item \texttt{retrieve}: find AI services on the network fulfilling a
  given specification.
\item \texttt{compose}: construct composite services fulfilling that
  specification.  Useful when no such AI services can be found.
\end{enumerate}

The experiment contains the same \texttt{incrementer}, \texttt{twicer}
and \texttt{halfer} services described in Section
\ref{network_idris_ai_services} with the important distinction that
their specifications now utilize dependent types.  For instance the
type signature of \texttt{twicer} becomes
\begin{minted}[mathescape]{idris}
twicer : Integer -> EvenInteger
\end{minted}
instead of
\begin{minted}[mathescape]{idris}
twicer : Integer -> Integer
\end{minted}
where \texttt{EvenInteger} is actually a shorthand for the following
dependent type
\begin{minted}[mathescape]{idris}
EvenInteger : Type
EvenInteger = (n : WFInt ** Parity n 2)
\end{minted}
that is a \emph{dependent pair} composed of a \emph{well founded
integer} of type \texttt{WFInt} and a dependent data structure,
\texttt{Parity} containing a proof that the first element of the pair,
\texttt{n}, is even.  More details on that can be found in
Section\nil{Add ref to Sam's work}.

For now our prototype of AI-DSL registry implements the
\texttt{retreive} function, which, given an Idris type signature,
searches through a database of AI services and returns one fulfilling
that type.  In that experiment the database of AI services is composed
of \texttt{incrementer}, \texttt{twicer}, \texttt{halfer}, the
\texttt{registry} itself and \texttt{compo}, a composite service using
previously listed services.

One can query each service via gRPC.  For instance querying the
\texttt{retreive} function of the \texttt{registry} service with the
following input
\begin{minted}[mathescape]{idris}
String -> (String, String)
\end{minted}
outputs
\begin{minted}[mathescape]{idris}
Registry.retreive
\end{minted}
which is itself.  Likewise one can query
\begin{minted}[mathescape]{idris}
Integer -> Integer
\end{minted}
which outputs
\begin{minted}[mathescape]{idris}
Incrementer.incrementer
\end{minted}
corresponding to the \texttt{Incrementer} service with the
\texttt{incrementer} function.
Next one can provide a query involving dependent types, such as
\begin{minted}[mathescape]{idris}
Integer -> EvenInteger
\end{minted}
outputting
\begin{minted}[mathescape]{idris}
Twicer.twicer
\end{minted}
Or equivalently provide the unwrapped dependent type signature
\begin{minted}[mathescape]{idris}
Integer -> (n : WFInt ** Parity n (Nat 2))
\end{minted}
retrieving the correct service again
\begin{minted}[mathescape]{idris}
Twicer.twicer
\end{minted}

At the heart of it is Idris.  Behind the scene the registry
communicates the type signature to the Idris REPL and requests, via
the {\texttt :search} meta function, all loaded functions matching the
type signature.  Then the registry just returns the first match.

Secondly, we can now write composite services with missing parts.  The
\texttt{compo} service illustrates this.  This service essentially
implements the following composition
\begin{minted}[mathescape]{idris}
incrementer . halfer . (Registry.retrieve ?type)
\end{minted}
Thus upon execution queries the registry to fill the hole with the
correct, according to its specification, service.

More details about this, including steps to reproduce it all, can be
found in the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/registry-dsl/README.md}{\texttt{README.md}}
under the
\href{https://github.com/singnet/ai-dsl/blob/master/experimental/registry-dsl/}{\texttt{experimental/simple-idris-services/service/}}
folder of the \href{https://github.com/singnet/ai-dsl/}{AI-DSL
  repository}~\cite{AIDSLRepo}.

\subsection{Objectives and achievements}

As shown above we were able to implement a proof-of-concept of an
AI-DSL registry.  Only the \texttt{retrieve} function was implemented.
The \texttt{compose} function still remains to be implemented,
although the \texttt{compo} service is somewhat halfway there, with
limitations, for instance the missing type, \texttt{?type}, was
hardwired in the code, \texttt{Integer -> EvenInteger}.  It should be
noted however that Idris is in principle capable of inferring such
information but more work is needed to more fully explore that
functionality.

Of course it is a very simple example, in fact the simplest we could
come up with, but we believe serves as a proof-of-concept, and
demonstrates that AI services matching, using dependent types as
formal specification language, is possible.

\subsection{Future work}

There a lot of possible future improvements for this work, in no
particular order
\begin{itemize}
\item Use structured data structures to represent type signatures
  instead of String.
\item Return a list of services instead of the first one.
\item Implement \texttt{compose} for autonomous composition.
\item Use real AI services instead of trivially simple ones.
\end{itemize}

Also, as of right now, the registry was implemented in
Python\footnote{because the SingularityNET example it is derived from
is written in Python, not because Python is thought to be the greatest
language for this purpose.}, querying Idris when necessary.  However
it is likely that this should be better suited to Idris itself.  Which
leads us to an interesting possibility, maybe the registry, and in
fact most (perhaps all) components and functions of the AI-DSL could
or should be implemented in the AI-DSL itself.

\chapter{AI-DSL Ontology (Kabir's work)}

\section{Description}

\subsection{Design requirements}
\label{design-requirements}

At the beginning of the current iteration of the AI-DSL project we had a round
of discussions about the high level functional and design requirements for
AI-DSL and its role in SingularityNET platfrom and ecosystem. The discussions
were based on
\cite{GoertzelGeisweillerBlog,singularitynet_foundation_phasetwo_2021}  and are
\href{https://github.com/nunet-io/ai-dsl-ontology/wiki/AI-DSL\%20requirements}{available
online} in their original form. Here is the summary of the preliminary design
requirements informed by those discussions:

\begin{itemize} \item AI-DSL is a language that allows AI agents/services
running on SinglarityNET platfrom to declare their capabilities and needs for
data to other AI agents in a rich and versatile machine readible form; This will
enable different AI agents to search, find data sources and other AI services
whithout human interaction; \item AI-DSL ontology defines data and service
(task) types to be used by AI-DSL. Requirements for the ontology are shaped by
the scope and specification of the AI-DSL itself; \end{itemize}

High level requirements for AI-DSL are:

\begin{description} \item[Extendability] The ontology of data types and AI task
types should be extendable in the sense that individual service providers /
users should be able to create new types and tasks and make them available to
the network. AI-DSL should be able to ingest these new types / tasks and
immediately be able to do the type-checking job. In other words, AI-DSL ontology
of types / tasks should be able to evolve. At the same time, extended ontologies
should relate to existing basic AI-DSL ontology in a clear way, allowing AI
agents to perform reasoning across the whole space of available ontologies
(which, at lower levels, may be globally inconsistent). In order to ensure
interoperability of lower level ontologies, AI-DSL ontology will define small
kernel / vocabulary of globally accessible grounded types, which will be
built-in into the platform at the deep level. Changing this kernel will most
probably require some form of voting / global consensus on a platfrom level.

  Therefore, it seems best to define AI-DSL Ontology and the mechanism of using
  it on two levels: \begin{itemize} \item \textit{The globally accessible
  vocabulary/root ontology of grounded types}. This vocabulary can be seen as
  immutable (in short and medium term) kernel. It should be extendible in the
  long term, but the mechanisms of changing and extending it will be quite
  complex, most probably involving theorectical considerations and/or a strict
  procedures of reaching global consensus within the whole platform (a sort of
  voting); \item \textit{A decentralized ontology of types and tasks} which each
  are based (i.e. type-dependent) on the root ontology/vocabulary, but can be
  extended in a decentralized manner -- in the sense that each agent in the
  platfrom will be able to define, use and share derived types and task
  definitions at its own discretion without the need of global consensus.
  \end{itemize}

  \item[Competing versions and consensus.] We want both consistency (for
  enabling deterministic type checking -- as much as it is possible) and
  flexibility (for enabling adaptation and support for innovation). This will be
  achieved by enforcing different restrictions for competing versions and
  consensus reaching on the two levels of ontology descrbed above:

  \begin{itemize} \item The globally accessible vocabulary / root ontology of
  grounded types will not allow for competing versions. In a sense, this level
  will be the true ontology, representable by a one and unique root /
  upper-level ontology of the network which users will not be able to modify
  directly; \item All other types and task definitions within the platform will
  be required to be derived from the root ontology (if they will want to be used
  for interaction with other agents); However, the platform whould not restrict
  the number of competing versions or define a global consensus  of types and
  task descriptions on this level. \item Furthermore, the ontology and the
  AI-DSL logic should allow for some variant of 'soft matching' which would
  allow to find the type / service that does not satisfy all requirements
  exactly, but comes as closely as available in the platform. \item At the
  lowest level of describing each instance of AI service or data source on the
  platfrom, AI-DSL shall allow maximum extentibility in so that AI service
  providers and data providers will be able to describe and declare their
  services in the most flexible and unconstrained manner, facilitating
  competition and cooperation between them. \end{itemize}

  \item[Code-level / service-level APIs.] It is important to ensure that the
  ontology is readable / writable by different components of the SingularityNET
  platform, at least between AI-DSL engine / data structures and each AI service
  separately. This is needed because some of the required descriptors of AI
  services will have to be dynamically calculated at the time of calling a
  service and will depend on the immediate context (e.g. price of service, a
  machine on which it is running, possibly reputation score, etc.). It is not
  clear at this point how much of this functionality will be possible (and
  practical) to implement on available dependently typed, ontology languages or
  even if it is possible to use single language. Even it if is possible to
  implement all AI-DSL purely on the current dependently typed language choice
  Idris, it will have to interface with the world, deal with indeterministic
  input from network and mutable states -- operations that may fail in run-time
  no matter how careful type checking is done during compile time
  \cite{brady_resource-dependent_2015}.

  Defining and maintaining code-level and service-level APIs will first of all
  enable interfacing SingularityNET agents to AI-DSL and therefore between
  themselves.

  \item[Key AI Agents properties] We can distinguish two somewhat distinct (but
  yet interacting) levels of AI-DSL Ontology AI service description level and
  data description level. It seems that it may be best to start building the
  ontology from the service level, because data description language is even
  more open-ended than AI description language, which is already open enough.
  Initially, we may want to include into the description of each AI service at
  least these properties: \begin{itemize} \item Input and output data structures
  and types \item Financial cost of service \item Time of computation \item
  Computational resource cost \item Quality of results \end{itemize} Most
  probably it is possible to express and reason about this data with Idris. It
  is quite clear however, that in order to enable interaction with and between
  SingularityNET agents (and NuNet adapters) all above properties have to be
  made accessible outside Idris and therefore supported by the code-level /
  service-level APIs and the SingularityNET platfrom in general.

\end{description}

\subsection{Domain model considerations}
\label{domain_model_considerations}

In order to attend to all high level design requirements. All levels of AI-DSL
Ontology should be developed simulatneously, so that we could make sure that the
work is aligned with the function and role of AI-DSL within SingularityNET
platform and ecosystem. We therefore use the "AI/computer-scientific"
perspective to ontology and ontology building -- emphasizing \textit{what an
ontologoy is for} -- rather than the "philosophical perspecive" dealing with
\textit{the study of what there is in terms of basic categories}
\cite{gruber_translation_1993,sep-logic-ontology}. Therefore we first propose
the  mechanism of how different levels (upper, domain and the leaf- (or
service)) of AI-DSL ontology will relate for facilitating interactions between
AI services on the platfrom.

Note, that design principles of such mechanism relate to the question how
abstract and consistent should relate to concrete and possibly inconsistent --
something that may need a deeper conceptual understanding than is attempted
during the project and presented here. We proceed in most practical manner for
proposing the AI-DSL ontology prototype, being aware that it may need to (and
possibly should) be subjected to more conceptual treatment in the future.

For a concrete domain model of AI-DSL ontologoy prototype we use the
\textit{Fake News
Warning}\footnote{\href{https://gitlab.com/nunet/fake-news-detection}{https://gitlab.com/nunet/fake-news-detection}}
application being developed by NuNet -- a currently incubated spinoff of
SingularityNET\footnote{\href{https://nunet.io}{https://nunet.io}}.

NuNet is the platfrom enabling dynamic deployment and up/down-scaling of
SingularityNET AI Services on decentralized hardware devices of potentially any
type. Importantly for the AI-DSL project, service discovery on NuNet is designed
in a way that enables dynamic construction of application-specific service
meshes from several SingularityNET AI services\cite{nunet_nunet_2021}. In order
for the service mesh to be deployed, NuNet needs only a specification of program
graph of the application. Note, that conceptually, construction of an
application from several independent containers is almost equivalent to
functionality explained in section~\ref{aidsl_registry} on AI-DSL Registry,
namely performance of matching and composition of AI services. This is the main
reason why we chose \textit{Fake News Warning} application as a domain model
for early development efforts of AI-DSL. However, we use this domain model
solely for the application-independent design of AI-DSL and attend to
its application specific aspects only as much as it informs the project.

The idea of dynamic service discovery is to enable application developers to
construct working applications (or at least their backends) by simply passing a
declarative definition of program graph to the special platform component
("network orchestrator") -- which then searches for appropriate SingularityNET
AI containers and connects them in to a single workflow (or workflows).
Suppose, that the backend of \textit{Fake News Warning} app consists of three
SingularityNET AI containers \textit{news\_score}, \textit{uclnlp} and
\textit{binary-classification}:

\begin{table}[H]
  \scriptsize
  \centering
  \begin{tabular}{p{0.15\linewidth}|p{0.2\linewidth}|p{0.2\linewidth}|p{0.2\linewidth}|p{0.1\linewidth}}
    \textbf{Leaf item} & \textbf{Description} & \textbf{Input} &
    \textbf{Output} &
    \textbf{Source}\\
    \hline
    binary-classification & A pre-trained binary classification model &
    English text of any length & 1 -- the text is categorized
    as fake; 0 -- text is categorized as not-fake & \textcopyright
    \href{https://gitlab.com/nunet/fake-news-detection/binary-classification}{NuNet}
    2021\\
    \hline
    uclnlp & Forked and adapted component of stance detection
    algorithm (\href{http://www.fakenewschallenge.org/#fnc1results}{FNC} third place
    winner) & Article title and text & Probabilities of the title \textit{agreeing},
    \textit{disagreeing}, \textit{discussing} or being \textit{unrelated} to the
    text & \textcopyright \href{https://github.com/uclnlp/fakenewschallenge}{UCL
    Machine Reading} 2017; \textcopyright
    \href{https://gitlab.com/nunet/fake-news-detection/uclnlp}{NuNet} 2021\\ \hline
    news-score & Calls dependent services, calculates overall result and sends them
    to the frontend & URL of the content to be checked & Probability that the content
    in the URL is fake & \textcopyright
    \href{https://gitlab.com/nunet/fake-news-detection/fake_news_score}{NuNet} 2021 \\
    \end{tabular}
  \captionsetup{width=0.7\linewidth}
  \caption{\label{tbl:fns_components}Description of each component of
  \texttt{Fake News Warning} application.}
\end{table}

Each component of application's backend is a SingularityNET AI Service
registered on the platfrom.  Note, that as SingularityNET AI services are
defined through their specification and their
metadata\cite{SNETDocumentationServiceSetup}. The main purpose of the AI-DSL
Ontology is to be able to describe SNet AI Services in a manner that would allow
them to search and match each other on the platfrom and compose into complex
workflows -- similarly to what is described in Section
\ref{network_idris_ai_services}. Here is a simple represenation of the program
graph of \textit{Fake News Warning} app:

\begin{figure}[h]
  \centering
    \begin{minted}[linenos,tabsize=2,breaklines, fontsize=\small]{json}
      "dag": {
        "news-score" : ["uclnlp","binary-classification"]
        }
    \end{minted}
    \captionsetup{width=0.7\linewidth}
    \caption{\label{lst:dag}A directed acyclic graph (DAT) of
      \textit{Fake News Warning} app
      prototype\cite{NuNetFakeNewsWarningAppRepo}.  Meaning that
      \texttt{news-score} depends on \texttt{uclnlp} and
      \texttt{binary-classification}.}
\end{figure}

The schematic representation of the \textit{Fake News Warning} app deployed as a
result of processing the DAG is depicted below. The addition of NuNet platfrom
to SingularityNET service discovery is that each service may be deployed on
different hardware environments, sourced by NuNet. When the application backend
is deployed, it can be accessed from the GUI interface, which in this case is a
Brave browser extension.

\begin{figure}[H]
  \begin{subfigure}[b]{0.50\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{../../../ontology/images/fake_news_detector.png}
    \captionsetup{width=0.8\linewidth}
    \caption{Schema of dependencies between backend components of the application
    (SingularityNET AI services potentially running on different machines).}
    \label{fig:fake_news_detector_schema}
  \end{subfigure}
  \begin{subfigure}[b]{0.50\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{../../../ontology/images/fake_news_detector_browser_extension.png}
    \captionsetup{width=0.8\linewidth}
    \caption{Brave browser extension which calls the backend of
    \textit{Fake News Warning} application on each invocation on new
    content displayed in browser tab.}
  \end{subfigure}
\end{figure}

We will use this application design principles as the domain model for the first
design of AI-DSL Ontology and the prototype.

\subsection{Ontology language and upper level ontology}

After discussing several choices of ontology languages and reusing existing
ontologies for designing AI-DSL ontology\footnote{See
\href{https://github.com/singnet/ai-dsl/discussions/18}{Reusing Existing
Ontologies} discussion on AI-DSL Github repository\cite{AIDSLRepo}}, we have opted to
use SUO-KIF as an ontology language~\cite{pease_standard_2009} and SUMO as an
upper-level ontology~\cite{NilesPease2001}. The main motivation for this choice
were the versatility of KIF/SUO-KIF (Knowledge Interchange Format) language,
which essentially allows to express first order logic statements in a simple
text format in terms of lisp-like syntax. Due to that, KIF can be easily
converted to other formats\cite{kalibatiene_survey_2011}. Also, a conversion to
Atomese -- the OpenCog's language also employing a list-like syntax -- has been
successfully attempted in the past\footnote{See
the \href{https://github.com/opencog/external-tools/tree/master/SUMO_importer}{SUMO
  Importer} in the OpenCog External Tools
repository\cite{ExternalToolsRepo}}. SUMO and the related ontology design tools
\cite{pease_sigma_2001} provide a convienent way for starting to design AI-DSL
Ontology levels and their relations.

\section{Objectives and achievements}

\section{Decentralized ontology}

In order to satisfy the \textit{extendibility} requirement of ontology design
(see~\ref{design-requirements}), we are proposing a notion and design of a \textit{decentralized ontology}, which enables us to work with globally consistent and locally incosistent components within the same mechanism of AI-DSL. Based on this design, the full ontology of \textit{Fake News Warning} application is constructed from a number of separate components, which operate at different level of decentralization. Table below describes each of these components.

\begin{table}[H]
%describe each kif file / level of ontology / consistent / inconsistent;
  \scriptsize
  \centering
  \begin{tabular}{p{0.24\linewidth}|p{0.24\linewidth}|p{0.24\linewidth}|p{0.24\linewidth}|}
    \textbf{Component} &
    \textbf{Description} &
    \textbf{Dependencies} &
    \textbf{Extendability}\\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/NuNet.kif}{NuNet.kif} &
    Defines classes to be used for describing each hardware resource eligible
    for running SingularityNET AI Services via NuNet platfrom;  &
    Merge.kif, SingularityNET.kif [,..]&
    Limited: versioning mechanism controlled by NuNet (to be defined) \\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/
    SingulairtyNet.kif}{SingularityNet.kif} &
    Defines global classes and types to be used for describing each
    SingularityNET AI Service &
    ComputerInput.kif, Merge.kif [,..]&
    Limited: versioning mechanism controlled by SingularityNET (to be defined)\\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/
    FakeNewsScore.kif}{FakeNewsScore.kif} &
    SingularityNET service responsible for constructing the whole backend of
    each \textit{Fake News Warning} application instance i.e. program graph
    (DAG) of the application.&
    SingularityNET.kif [,..] &
    Fully decentralized: defined by application developers; Since \textit{Fake News
Warning} application is open source, any developer can for it and define it
otherwise; Technically, this would be a different application.\\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/fnsBinaryClassifier.kif}{fnsBinaryClassifier.kif} &
    A pre-trained binary classification model for fake news detection &
    SingularityNET.kif [,..] &
    Fully decentralized: defined by each algorithm developer independently.
    Technically, from the platfrom perspective, these will be different
    algorithms. \\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/
    NuNetEnabledComputer.kif}{NuNetEnabledComputer.kif} &
    Each NuNet enabled hardware resource will have to be described accordingly
    when onboarded to NuNet platfrom &
    NuNet.kif &
    Fully decentralized: independently defined by the owner of a hardware
    resource \\
    \hline
    \href{https://github.com/singnet/ai-dsl/blob/master/ontology/
    uclnlp.kif}{uclnlp.kif} &
    Forked and adapted component of stance detection algorithm by UCL Machine Reading group &
    SingularityNET.kif [,..] &
    Fully decentralized: defined by each algorithm developer independently.
    Technically, from the platfrom perspective, these will be different
    algorithms \\
    \end{tabular}
  \captionsetup{width=0.7\linewidth}
  \caption{\label{tbl:all_kif_files}Description of each component of the AI-DSL Ontology prototope and links to related KIF files.}
\end{table}

\section{The mechanism of constructing ontology}

%Versions / voting / communication

\begin{enumerate}
  \item Upper level SUMO ontology
  (\href{https://github.com/ontologyportal/sumo/blob/master/Merge.kif}{Merge.kif});
  \item Middle level SUMO ontology
(\href{https://github.com/ontologyportal/sumo/blob/master/Mid-level-ontology.kif}{Mid-level-ontology.kif});
  \item Distributed computing hardware domain ontology in SUO-KIF
(\href{https://github.com/ontologyportal/sumo/blob/master/QoSontology.kif}{QoSontology.kif});
\end{enumerate}

\begin{figure}[h] \centering \begin{subfigure}[b]{1\textwidth} \centering
\includegraphics[width=\textwidth]{../../../ontology/images/SNetAIService.png}
\caption{Class dependencies of the SNetAIService subclass within the ontology.}
\label{fig:SNetAIService hierachy} \end{subfigure}
\begin{subfigure}[b]{1\textwidth} \centering
\includegraphics[width=\textwidth/4]{example-image-a} \caption{Class
dependencies of the SNetAppBackend subclass (workflow definition) within the
ontology.} \label{fig:tSNetAppBackend} \end{subfigure} \caption{Class
dependences of AI-DSL Ontology (SingularityNet.kif) }
\label{fig:SingularityNet.kif} \end{figure}

\kabir[inline]{using ontology for agent communication in decentralized computing
systems, based on \cite{YvesHellenschmidt2002}}

\subsection{Leaf ontology (fake-news-detection workflow definition)}

The prototype will be the fake-news-detector leaf ontology based on the above
listed upper and middle ontologies (SUMO) and domain ontologies of computer
hardware and software.

\begin{figure}[H]
\inputminted[firstline=2, lastline=12, linenos,tabsize=2,breaklines, fontsize=\small]{scm}{../../../ontology/SingularityNet.kif}
\caption{\label{lst:singularitynet}Contents of SingularityNet.kif}
\end{figure}
\nil[inline]{Very cool, I think we want to include the (subclass
  Boolean DataType) lines, etc, as well, as they start touching the
  what matters}
\kabir[inline]{Yes, we will. The kif files are quite large and I will have to
think how to include the most important parts to test;
but of course i will provide links to them here.}

\section{Future work}

\begin{itemize}
  \item In the long term, it may be ideal to develop a converter for converting
it to KIF, since OWL may be representable in KIF
\cite{martin_translations_nodate}  using \href{OWL
API}{https://github.com/owlcs/owlapi}; For the purpose of the ontology
prototype, we will manually select parts of the this ontology in order to build
the prototype and write them in SUO-KIF format;
  \item Combining ontology with Idris
\kabir{It would be good to have a section explaining ideas about that,
but I cannot do this alone, so probably the best is to reserve it for the end of
the monht, when all the other aspects of AI-DSL project (including Idris) are
explained.}
\end{itemize}


\chapter{Eman's work}

\chapter{Sam's work}



\bibliographystyle{splncs04}
\bibliography{local}

\end{document}
